
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'r').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {u:i for i, u in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '<': 23,\n",
       " '>': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " '`': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '|': 82,\n",
       " '}': 83}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1',\n",
       "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
       "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
       "       'w', 'x', 'y', 'z', '|', '}'], dtype='<U1')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ..., 30, 39, 29])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"From fairest creatures we desire increase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_stanza = \"\"\"From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(part_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_seq = len(text)//(seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ds = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(char_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_ds.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(seq):\n",
    "    input_txt = seq[:-1]  # Hello my nam\n",
    "    target_txt = seq[1:]  # ello my name\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "source": [
    "for input_txt, target_txt in dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(\"\".join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    print(\"\".join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(128, 120), dtype=tf.int32, name=None), TensorSpec(shape=(128, 120), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size=vocab_size, embed_dim=embed_dim, rnn_neurons=rnn_neurons, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (128, None, 64)           5376      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (128, None, 1026)         3361176   \n",
      "                                                                 \n",
      " dense (Dense)               (128, None, 84)           86268     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 120, 84) <==(batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_preds = model(input_example_batch)\n",
    "    print(example_batch_preds.shape, '<==(batch_size, sequence_length, vocab_size)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 120, 84), dtype=float32, numpy=\n",
       "array([[[ 0.00691227, -0.00157334, -0.00189828, ..., -0.00190158,\n",
       "         -0.00188533, -0.00445857],\n",
       "        [ 0.0106821 , -0.00147247, -0.00255075, ..., -0.00331371,\n",
       "         -0.00240403, -0.00700452],\n",
       "        [ 0.01252586, -0.00096304, -0.00249058, ..., -0.00427628,\n",
       "         -0.00272396, -0.00837313],\n",
       "        ...,\n",
       "        [ 0.01250521, -0.00165469, -0.00210885, ..., -0.00408558,\n",
       "         -0.00085695, -0.00765684],\n",
       "        [ 0.01398135,  0.00329351,  0.00319183, ..., -0.0025824 ,\n",
       "          0.00141499, -0.00340228],\n",
       "        [ 0.00478786,  0.00169462, -0.00013691, ...,  0.00580987,\n",
       "         -0.00209039, -0.00565508]],\n",
       "\n",
       "       [[ 0.00691227, -0.00157334, -0.00189828, ..., -0.00190158,\n",
       "         -0.00188533, -0.00445857],\n",
       "        [ 0.00623751,  0.00043518,  0.00484299, ...,  0.001547  ,\n",
       "         -0.00240841, -0.00162232],\n",
       "        [ 0.00513735,  0.00186767, -0.00290301, ..., -0.0046595 ,\n",
       "         -0.00079769, -0.00087963],\n",
       "        ...,\n",
       "        [-0.00067151, -0.00161333, -0.00074458, ..., -0.00223286,\n",
       "         -0.00605361, -0.0011722 ],\n",
       "        [-0.00142685,  0.00116894, -0.00093059, ...,  0.00835403,\n",
       "          0.00305033,  0.0023576 ],\n",
       "        [ 0.00038859, -0.00015115, -0.00178436, ...,  0.00049819,\n",
       "         -0.00232552, -0.00288718]],\n",
       "\n",
       "       [[-0.00765175, -0.00288649, -0.00281272, ..., -0.00439089,\n",
       "         -0.0012964 , -0.00092108],\n",
       "        [ 0.00391442, -0.0014213 , -0.00318702, ..., -0.00423195,\n",
       "         -0.0025486 , -0.00490308],\n",
       "        [-0.00257961,  0.00449369, -0.00268685, ..., -0.00360677,\n",
       "         -0.00717127, -0.00129499],\n",
       "        ...,\n",
       "        [ 0.01067821, -0.00049677, -0.00248433, ..., -0.00093058,\n",
       "         -0.00221209, -0.00401421],\n",
       "        [-0.00033769,  0.00028641, -0.00097303, ...,  0.00162939,\n",
       "         -0.00235668, -0.00083418],\n",
       "        [-0.00160367,  0.00428859, -0.00011747, ..., -0.00417579,\n",
       "         -0.00794718, -0.00244187]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.00691227, -0.00157334, -0.00189828, ..., -0.00190158,\n",
       "         -0.00188533, -0.00445857],\n",
       "        [-0.00105243, -0.00201466, -0.00060854, ...,  0.00203465,\n",
       "          0.00297946, -0.00581373],\n",
       "        [ 0.00397634,  0.00263013,  0.00547168, ...,  0.01031184,\n",
       "          0.00174045, -0.00434384],\n",
       "        ...,\n",
       "        [ 0.00831847,  0.00250774, -0.00090111, ..., -0.0097412 ,\n",
       "         -0.00302674, -0.00377823],\n",
       "        [ 0.00214766,  0.00374211, -0.00194928, ...,  0.00379324,\n",
       "          0.00338094, -0.00113127],\n",
       "        [-0.00480651,  0.00089095, -0.00226942, ...,  0.00512827,\n",
       "          0.00020169, -0.00087894]],\n",
       "\n",
       "       [[ 0.00691227, -0.00157334, -0.00189828, ..., -0.00190158,\n",
       "         -0.00188533, -0.00445857],\n",
       "        [ 0.0106821 , -0.00147247, -0.00255075, ..., -0.00331371,\n",
       "         -0.00240403, -0.00700452],\n",
       "        [ 0.01252586, -0.00096304, -0.00249058, ..., -0.00427628,\n",
       "         -0.00272396, -0.00837313],\n",
       "        ...,\n",
       "        [ 0.00139498,  0.00459516,  0.00195028, ..., -0.00402168,\n",
       "         -0.00330402, -0.00387948],\n",
       "        [-0.00103072,  0.00578308,  0.00282112, ...,  0.00709175,\n",
       "          0.00270977, -0.00113579],\n",
       "        [ 0.00223477,  0.00398982,  0.00608154, ...,  0.01373963,\n",
       "          0.00074991, -0.00262985]],\n",
       "\n",
       "       [[ 0.00237473,  0.00014808,  0.00114756, ...,  0.00601733,\n",
       "          0.00010947, -0.00272319],\n",
       "        [ 0.00827676, -0.00158112, -0.00215898, ...,  0.00081373,\n",
       "         -0.0013764 , -0.00532708],\n",
       "        [ 0.01662866, -0.00021446, -0.00390957, ...,  0.00510662,\n",
       "         -0.00254139, -0.00996453],\n",
       "        ...,\n",
       "        [ 0.00311804,  0.00014124, -0.00174079, ...,  0.00384908,\n",
       "          0.0026862 , -0.00961432],\n",
       "        [ 0.00621597,  0.00424607,  0.00499417, ...,  0.01034032,\n",
       "          0.00184316, -0.00629604],\n",
       "        [ 0.00092781,  0.00341152,  0.00236334, ...,  0.00251898,\n",
       "         -0.00467719, -0.00419633]]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_preds[0], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(120, 1), dtype=int64, numpy=\n",
       "array([[28],\n",
       "       [64],\n",
       "       [31],\n",
       "       [60],\n",
       "       [49],\n",
       "       [62],\n",
       "       [62],\n",
       "       [12],\n",
       "       [23],\n",
       "       [16],\n",
       "       [60],\n",
       "       [53],\n",
       "       [70],\n",
       "       [18],\n",
       "       [ 8],\n",
       "       [66],\n",
       "       [38],\n",
       "       [77],\n",
       "       [28],\n",
       "       [82],\n",
       "       [80],\n",
       "       [77],\n",
       "       [69],\n",
       "       [64],\n",
       "       [11],\n",
       "       [54],\n",
       "       [53],\n",
       "       [83],\n",
       "       [80],\n",
       "       [76],\n",
       "       [24],\n",
       "       [34],\n",
       "       [12],\n",
       "       [15],\n",
       "       [28],\n",
       "       [45],\n",
       "       [82],\n",
       "       [80],\n",
       "       [51],\n",
       "       [62],\n",
       "       [41],\n",
       "       [72],\n",
       "       [57],\n",
       "       [31],\n",
       "       [47],\n",
       "       [18],\n",
       "       [81],\n",
       "       [65],\n",
       "       [20],\n",
       "       [74],\n",
       "       [29],\n",
       "       [25],\n",
       "       [46],\n",
       "       [ 2],\n",
       "       [57],\n",
       "       [34],\n",
       "       [15],\n",
       "       [79],\n",
       "       [34],\n",
       "       [43],\n",
       "       [13],\n",
       "       [ 8],\n",
       "       [23],\n",
       "       [51],\n",
       "       [81],\n",
       "       [79],\n",
       "       [11],\n",
       "       [38],\n",
       "       [48],\n",
       "       [19],\n",
       "       [49],\n",
       "       [58],\n",
       "       [11],\n",
       "       [ 5],\n",
       "       [29],\n",
       "       [55],\n",
       "       [ 7],\n",
       "       [16],\n",
       "       [ 1],\n",
       "       [69],\n",
       "       [28],\n",
       "       [ 9],\n",
       "       [63],\n",
       "       [72],\n",
       "       [45],\n",
       "       [14],\n",
       "       [78],\n",
       "       [68],\n",
       "       [66],\n",
       "       [ 6],\n",
       "       [ 0],\n",
       "       [19],\n",
       "       [ 3],\n",
       "       [20],\n",
       "       [16],\n",
       "       [40],\n",
       "       [71],\n",
       "       [82],\n",
       "       [25],\n",
       "       [19],\n",
       "       [45],\n",
       "       [23],\n",
       "       [73],\n",
       "       [71],\n",
       "       [54],\n",
       "       [70],\n",
       "       [50],\n",
       "       [70],\n",
       "       [67],\n",
       "       [36],\n",
       "       [30],\n",
       "       [34],\n",
       "       [10],\n",
       "       [48],\n",
       "       [23],\n",
       "       [22],\n",
       "       [24],\n",
       "       [74],\n",
       "       [10],\n",
       "       [59]], dtype=int64)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat to not be a list of lists\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 64, 31, 60, 49, 62, 62, 12, 23, 16, 60, 53, 70, 18,  8, 66, 38,\n",
       "       77, 28, 82, 80, 77, 69, 64, 11, 54, 53, 83, 80, 76, 24, 34, 12, 15,\n",
       "       28, 45, 82, 80, 51, 62, 41, 72, 57, 31, 47, 18, 81, 65, 20, 74, 29,\n",
       "       25, 46,  2, 57, 34, 15, 79, 34, 43, 13,  8, 23, 51, 81, 79, 11, 38,\n",
       "       48, 19, 49, 58, 11,  5, 29, 55,  7, 16,  1, 69, 28,  9, 63, 72, 45,\n",
       "       14, 78, 68, 66,  6,  0, 19,  3, 20, 16, 40, 71, 82, 25, 19, 45, 23,\n",
       "       73, 71, 54, 70, 50, 70, 67, 36, 30, 34, 10, 48, 23, 22, 24, 74, 10,\n",
       "       59], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the input seq: \n",
      "\n",
      "   much love as she finds. There is more owing her than is paid; and\n",
      "    more shall be paid her than she'll demand.\n",
      "  ST\n",
      "\n",
      "\n",
      "Next Char in predictions\n",
      "CiFeXgg1<5e]o7,kMvC|yvni0_]}yu>I14CT|yZgPqbFV7zj9sD?U!bI4xIR2,<Zzx0MW8Xc0'D`)5 nC-hqT3wmk(\n",
      "8\"95Op|?8T<rp_oYolKEI.W<;>s.d\n"
     ]
    }
   ],
   "source": [
    "print(\"Given the input seq: \\n\")\n",
    "print(\"\".join(ind_to_char[input_example_batch[0]]))\n",
    "print(\"\\n\")\n",
    "print(\"Next Char in predictions\")\n",
    "print(\"\".join(ind_to_char[sampled_indices ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "351/351 [==============================] - 723s 2s/step - loss: 2.5138\n",
      "Epoch 2/30\n",
      "351/351 [==============================] - 736s 2s/step - loss: 1.7236\n",
      "Epoch 3/30\n",
      "351/351 [==============================] - 707s 2s/step - loss: 1.4559\n",
      "Epoch 4/30\n",
      "351/351 [==============================] - 696s 2s/step - loss: 1.3371\n",
      "Epoch 5/30\n",
      "351/351 [==============================] - 644s 2s/step - loss: 1.2753\n",
      "Epoch 6/30\n",
      "351/351 [==============================] - 645s 2s/step - loss: 1.2356\n",
      "Epoch 7/30\n",
      "351/351 [==============================] - 630s 2s/step - loss: 1.2065\n",
      "Epoch 8/30\n",
      "351/351 [==============================] - 649s 2s/step - loss: 1.1830\n",
      "Epoch 9/30\n",
      "351/351 [==============================] - 656s 2s/step - loss: 1.1632\n",
      "Epoch 10/30\n",
      "351/351 [==============================] - 660s 2s/step - loss: 1.1463\n",
      "Epoch 11/30\n",
      "351/351 [==============================] - 670s 2s/step - loss: 1.1306\n",
      "Epoch 12/30\n",
      "351/351 [==============================] - 655s 2s/step - loss: 1.1170\n",
      "Epoch 13/30\n",
      "351/351 [==============================] - 651s 2s/step - loss: 1.1040\n",
      "Epoch 14/30\n",
      "351/351 [==============================] - 647s 2s/step - loss: 1.0911\n",
      "Epoch 15/30\n",
      "351/351 [==============================] - 651s 2s/step - loss: 1.0798\n",
      "Epoch 16/30\n",
      "351/351 [==============================] - 644s 2s/step - loss: 1.0689\n",
      "Epoch 17/30\n",
      "351/351 [==============================] - 674s 2s/step - loss: 1.0582\n",
      "Epoch 18/30\n",
      "351/351 [==============================] - 670s 2s/step - loss: 1.0480\n",
      "Epoch 19/30\n",
      "351/351 [==============================] - 678s 2s/step - loss: 1.0390\n",
      "Epoch 20/30\n",
      "351/351 [==============================] - 693s 2s/step - loss: 1.0298\n",
      "Epoch 21/30\n",
      "351/351 [==============================] - 747s 2s/step - loss: 1.0221\n",
      "Epoch 22/30\n",
      "351/351 [==============================] - 710s 2s/step - loss: 1.0146\n",
      "Epoch 23/30\n",
      "351/351 [==============================] - 677s 2s/step - loss: 1.0073\n",
      "Epoch 24/30\n",
      "351/351 [==============================] - 691s 2s/step - loss: 1.0011\n",
      "Epoch 25/30\n",
      "351/351 [==============================] - 702s 2s/step - loss: 0.9956\n",
      "Epoch 26/30\n",
      "351/351 [==============================] - 688s 2s/step - loss: 0.9907\n",
      "Epoch 27/30\n",
      "351/351 [==============================] - 700s 2s/step - loss: 0.9859\n",
      "Epoch 28/30\n",
      "351/351 [==============================] - 783s 2s/step - loss: 0.9816\n",
      "Epoch 29/30\n",
      "351/351 [==============================] - 867s 2s/step - loss: 0.9779\n",
      "Epoch 30/30\n",
      "351/351 [==============================] - 691s 2s/step - loss: 0.9745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1facc1fbd00>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shakespeare1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "model.load_weights('shakespeare1.h5')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (1, None, 64)             5376      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (1, None, 1026)           3361176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 84)             86268     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters.\n",
    "  '''\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char_to_ind[s] for s in start_seed]\n",
    "\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    " \n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "\n",
    "  for i in range(num_generate):\n",
    "\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(ind_to_char[predicted_id])\n",
    "\n",
    "  return (start_seed + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowers.\n",
      "  FORD. Ay; so I do this.\n",
      "  PACILLO. No foul humourings I had not been in a chink\n",
      "    Richer than shallow friends, that, when false Edward\n",
      "    Is imprison'd and driven to thy father's seat\n",
      "    Wherein my sons, so high a debit of thee,\n",
      "    For then I long to see the envious foot.\n",
      "    Your will and blushes, that man with some sweat, guest\n",
      "    Should it go forward; yonder latch it come\n",
      "    on her sister or two less, I know the riotous time.\n",
      "    But as an I, is sheep, my double queen.\n",
      "    And brings me here that all the world should be?\n",
      "  EMILIA. Hou wood!' I will my noble knag-\n",
      "    An Admirable, discourse, Sungua, Hermia,  \n",
      "    If he were made. Good joy, I scarcely thus: my master\n",
      "    Is she but whiteness in a silky kiss.\n",
      "    O good I gave thy valour. What is yours?\n",
      "  VALENTINE. Hark you so good.\n",
      "    If I more wish I do this business\n",
      "    We will fight much to England being alive,\n",
      "    But steel it off. Come, take me to work you; these\n",
      "    bears his holy adversaries.\n",
      "  FIRST LORD. My lord!\n",
      "  PROTEUS. Wherefore ecquases? Desires young Montforth.\n",
      "    Look, like a traitor, lords, only love you, break,\n",
      "    If thy unksome cruel prologue that may find it so,\n",
      "    Such as the tide-phouse well or nay in peace;\n",
      "    For love, quarters, fair Moor, Pyramus and His lisp, ANTONIO. I am a\n",
      "    king'o?  \n",
      "  SIR TOBY. [Aside] but he pass hold of you are setter\n",
      "    The pasts, and dancer stays for Richer mind;\n",
      "    Finely put on friend Rome' a banishmomor\n",
      "    Were smiling in his challenge. Go you to my\n",
      "    tongue.\n",
      "  EVANS. Leave my bones!\n",
      "  BLANCH. So.\n",
      "  HELENUT. Now, if you did the time of sight and she\n",
      "    I grow from Tartheserter.\n",
      "  SUFFOLK. I woubled spice. So is Licio and Madam\n",
      "  \n",
      "  SHALLOW. Nay, if you love me from his friends to fight\n",
      "    And for my conscience, under heaven so far\n",
      "    On him in achinance or your thief:\n",
      "    But some dectimes of us did raise as these,\n",
      "    To drink a mortal womb of sovereignty;\n",
      "               And cry me 'That's the reason but thus said,\n",
      "    That nosely finch Ajax. How didst thou make us say\n",
      "    Shed me a cannon it; for at your partner\n",
      "    Achieve Gin go with me?\n",
      "    Alas, poorl? Would that we fear?\n",
      "    Is this the true person and the neighbourship? I\n",
      "    will go sleeping. O, she depos'd!\n",
      "  ACHILLES. How have you forc'd unto a hollow but wax;\n",
      "      But with since we abuceth him, men hurt uloed.\n",
      "  DUCHESS. I know the hound that Rome is dead:\n",
      "    Nor God in heaven and men with him that is shar'd\n",
      "    As he. His pride, love's shame in hand, and danger\n",
      "    Upon them foolish and done well consume.\n",
      "    What dost thou speak for your tree that I detest,\n",
      "    I dare clif my daughter to a kiss in all,\n",
      "    He swore to this sorrow do you speak with you.\n",
      " ge each other's painted\n",
      "    Which this fragments.\n",
      "  PROVOST. may she be with folly for a sea-man-self,\n",
      "    for a foot or of Urress Con. Launcelot! thou shalt not answer Jew.\n",
      "  GHOSIUS. All men's ray, I should have so.\n",
      "  IAGO. And will bring thee allegiance for a peace,\n",
      "    And construe the foul waves with me and me\n",
      "    Something else being still, and she must royal,\n",
      "    But that the company more of their places;\n",
      "    Our countrymen we miss to quench the Moor;\n",
      "    In such divorc'd sweet sir, that thou art gone.\n",
      "  KING RICHARD. Ay, those a swarm on formallable is asleep, forswearing it as embraces\n",
      "    Were foul in telling to survize him death.\n",
      "  LEPIDUS. But will you, uncle?\n",
      "  VALENTINE. I warrant her.  \n",
      "  KING PHILIP. I did; forblood more conspires amends,\n",
      "    Subdue me not.\n",
      "                                                    [Aside to AGAMEMNOND trembles of a burnish'd time,\n",
      "    And that's with hope, traitor, to make both treasons.\n",
      "    Say, fair or no! Art thou deliver thee\n",
      "    That Troy, and arms at wit and songs for audience.  \n",
      "                        Exit\n",
      "  JULIA. Go to, I know, and lay in pity of the aim\n",
      "    As true as rattery to their kind enjoys\n",
      "    With so offended him.\n",
      "  TIMON. Farewell, my lords. Now the hour wives for fum\n",
      "    Holds of your thoughts- be a tall shipp'd fortune of my love diligence\n",
      "    As shall poor Harryom hath stale the dalignaimart, they say so.\n",
      "  LUCENTIO. Hast thou belew'st me with more smalect of her\n",
      "    fellows while I said so. He cannot choose but any past-\n",
      "    He has a made perforce my knowledge,\n",
      "    And of the dismal to my mistress. Alas, how likest thou\n",
      "    Too soon as fresh fitules the morning's daughter.\n",
      "  ARIEL. By God's cack, I say; and my father go two in a saint, but, mine host, any but I\n",
      "    speak with you.\n",
      "  OLIVIA. Out, my hearty!\n",
      "    I will but signify in my neglect of his princely backs,\n",
      "    And with more moment pay and conclusing s\n",
      "    My sur loves you to it virtue and beying\n",
      "    with his newn. How I may chide awhile a round about her sheep?\n",
      "  GAUNT. Let this fall do more lave on Mistress\n",
      "    And bade the flouring eld with William villains.\n",
      "    Come, come, whate'er it dote-on my unqueised dry,  \n",
      "    Then buried in bolters be some rmy. This Horbfor is waving.          Exeunt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ACT V. SCENE 6.\n",
      "Another part of die senseless every way,\n",
      "    And out a speedy ancestion gentlemen,  \n",
      "    Hully, sir.\n",
      "  FABIAN. Come, sir, I prithee leadnt I wish HERMIONE. You are a most profound weary sinly herd doth native words\n",
      "    From sign with some dread men may be a Braken, and where I\n",
      "    hear their caps.\n",
      "\n",
      "                 Enter THILO, Juliet, Egromaid.\n",
      "  \n",
      "\n",
      "    Cht sure, my lord,  \n",
      "    If they had sent to serve most wise assaulted to the quick'ning in King.\n",
      "            But I must look to is bur\n",
      "    your master.\n",
      "  KATHERINA. [Opoden thee, MAY BE\n",
      "DISTRIBUTNONIC BERED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\n",
      "WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\n",
      "DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\n",
      "PERSONAL USEPSOONE. Why, what's the matter,\n",
      "    The King in all atone my love doth spend his horns.\n",
      "  TITUS. Peace! sway, you shall. Repeting in sad woman;\n",
      "    I saw her woman's service of my power.\n",
      "  Rom. Would Donal cries,\n",
      "             Must needs be sworn to quit,\n",
      "    Beat me how thy prologuers were not mad,\n",
      "    And usus the lover, and not monstrous to me,\n",
      "    And ventures here\n",
      "    The burn more in him.\n",
      "  DUKE. A stand now!\n",
      "  POLIXENES. Stay, Hector? What says she that you love me?\n",
      "  THERSITE. I wish your pleasure.\n",
      "  FABITHA. You come to wait on you, sir, 'tis his night.\n",
      "    [Knockings] and fools of him,\n",
      "    Has better earth.\n",
      "  ALONSO. I person me. I would I make\n",
      "    the worst with thee the friar? Why, very much that she has young you\n",
      "    To even here in't. I see the young petition to the curland\n",
      "    That are the King,\n",
      " with us?\n",
      "  SEBASTIAN. One fever of the walls, d leave away.\n",
      "  ANTONIO. Never must kill me. Look up under So.\n",
      "  HELENU. Let it stretch it out.\n",
      "  BASSANIO. How dost thou then to what I say,\n",
      "    I'll give us among trembly to affect him here,\n",
      "    And quit hold on his patron, which not\n",
      "    present this day from such cloud. Well, sweet fellows, stay with child\n",
      "    In difference of Helenus; men did hear again.\n",
      "  ARMADO. The King shall be my pity; now that will force you\n",
      "    Worthy Caspio.\n",
      "  so.\n",
      "\n",
      "                 Enter ACHILLES and PATRONIC ross impudent soft a\n",
      "    man as lief there is a youth's from forty part, hat. Let us follow it; which is wanting here, his ships crack'd.\n",
      "  PANTHINO. Bear the women on your charact,\n",
      "    Immortal fleet to seeing like.\n",
      "  LORD. Tut! howlong and souls!) use example.\n",
      "  BATTIMAN. Why I proede I will bull be scarf'd up in ashese of the Archbishop and time\n",
      "    We should friends will be lost. So, madam, will your Grace\n",
      "    Have done, an answer good do yence on eart!\n",
      "    Trust me? Why then, your uncle would have course,\n",
      "    Bid mischief Parthian, have we             Enter a MESSENGER\n",
      "\n",
      "    How now?\n",
      "  VALENTINE. None is more sail.\n",
      "  LAUNCELOT. I shall harten till  and embroched a piece of thine;\n",
      "    He may aspect his coming for't, droit ou be made to be false at alone. Most certainly\n",
      "    enough of change of taking.\n",
      "  CITIZEN. Why stay thou here? Be it so, brings do me your\n",
      "    master, sir, a friend.\n",
      "  FIRST SENATOR. Tranic the pow'rs of blood-\n",
      "    The proudest of mine eye and mind-delil or speedy approbation.\n",
      "    My sighs and general joy upon yond old;\n",
      "    What meritation of eyes she shall turn to moe.\n",
      "  MARIA. Your mean to see the DUKE'S palace.\n",
      "        Enter KING EDWARD, GLOUCESTER,           Is his several daughters loves.\n",
      "    My lord except his love-son, Pinvar!\n",
      "  TIMON. Rat withal! Was this? Know, Captain? O, what means that love.\n",
      "  CLOWN  Which is, to prove Mistress Bariaro over of our tyranny. O Caesar, drink, and\n",
      "    follow this:\n",
      "    But with your speechless sum of her eye have been!\n",
      "\n",
      "                 Enter DUKE FRENCH, invising walk,\n",
      "    Asknowledge me an honour of my mind it spirit\n",
      "    As milk-whispering at my love a troops\n",
      "    Of all respects. I cannot beguil'd. Search aution stands 't,\n",
      "    As will to our Duke Humphrey; 'twas to see\n",
      "    The tyrial-place. If we may be.\n",
      "    Three thousand'st that I could not prove?\n",
      "    A horse with painted music by my finger?\n",
      "  WIDOW. A woman living lutes, largely even soft' have.\n",
      "                          Exeunt BAPTISTAMO in  The septlection of nobleman of Salisbury!\n",
      "  GLOUCESTER. Ay, sir, and in good crowns. Get you the King.\n",
      "  APEMANTUS. Ho! de?\n",
      "    Symust not be thy nurse! Why should I say you?\n",
      "  Jul. Shall I send tongue to steel,\n",
      "    Shall chide to thee factor, or one meas,\n",
      "    And that should profane, with ranks and durs.\n",
      "    And then the keeper'st wholl security.\n",
      "  PETRUCHIO. The Spead, I me; thou hast upon mincess!'\n",
      "                                   Exeunt all but EPHESUS. The life forbid\n",
      "  KING RICHARD. The peril of my cause is found,\n",
      "    And go with him, look. But you, sir,\n",
      "    Here I the purpose. Nor it be a Creile,' I will be rank for foul tigers\n",
      "    To strive to policy.\n",
      "\n",
      "      Enter CHARLES, MOORSTIPHOLUS OF KING\n",
      "\n",
      "  WIFL. Ayive; his youth is venerato.\n",
      "  OTHELLO. The French are of 'em?\n",
      "    Give signal you in this house, yield me me\n",
      "          Thus I do harm me down, effectual consciences\n",
      "    Hath she be\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 'flower', gen_size=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
